in chapter 4
 - stringtype
 - biginttype
 - structtype
 these map to spark languages like string, integer, double
 
but when it comes to dataframe, do not create strings or integers but manipulates the data for you by manipulating the Row object
=> dataframes are actually datasets of type Row

"encoder" maps the domain-specific type T to sparks internal type system

when you use the dataset api, spark converts the spark row format to the object you specified( a case class or java class)
this might slow down the operations but can porvide more flexibility.
( but not extreme as switching programming languages )

* 
