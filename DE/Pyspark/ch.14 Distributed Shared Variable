second kind of low-level Api : distributed shared variable
 1. broadcast variable : save a large value on all the worker nodes and reuse it across many Spark actions without re-sending it to the cluster
 2. accumulators : let user add together data from all the tasks into a shared result
these two are variables you can use in user-defined functions

* Broadcast Variables : a way to share an immutable value around the cluster without encapsulating that variable in a function closure.
 - normal way to use a variable is to simply reference it in the function closures
  => inefficient for large variable such as a lookup table or a ml model.
 - broad cast variables are shared, immutable variables that are cached on every machine in the cluster instead of serialized with every single task.
 - ex) a list of words or values
  1)
    in Scala
    val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple"
    .split(" ")
    val words = spark.sparkContext.parallelize(myCollection, 2)

    in python
    my_collection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")
    words = spark.sparkContext.parallelize(my_collection, 2)
  
  2)  right join 
    in Scala
    val supplementalData = Map("Spark" -> 1000, "Definitive" -> 200, "Big" -> -300, "Simple" -> 100)

    in Python
    supplementalData = {"Spark":1000, "Definitive":200, "Big":-300, "Simple":100}

  3) broadcast this structure across spark and reference it by using suppBroadcast
    (valus is immutable and is lazily replicated across all nodes in the cluster when we trigger an action
    in scala
    val suppBroadcast = spark.sparkContext.broadcast(supplementalData)

    in python
    suppBroadcast = spark.sparkContext.broadcast(supplementalData)

  4) reference this variable via the varialbe method
    in scala
    suppBroadcast.value

    in python
    suppBroadcast.value

  5) transform RDD using this value 
   - create a key-value pair according to the value we might have in the map
   in scala
   words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))
    .sortBy(wordPair => wordPair._2)
    .collect()
   
   in python
   words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\
    .sortBy(lambda wordPair: wordPair[1])\
    .collect()
   => [('Big', -300),
        ('The', 0),
        ...
        ('Definitive', 200),
        ('Spark', 1000)]


* Accumulators : a way of updating a value inside of a variety of transformations and propagating that value to the driver node in an efficient and fault-tolerant way
 - provide a mutable variable that a Spark cluster can safely update on a per-row basis
  - can use this for debugging purposes or to create low-level aggregation
 - variables that are added to only through an associative and commutative operation and can therefore be supported in parallel
 - spark guarantees that each task's update to the accumulator will be applied only once, meaning that restarted tasks will not update the value
 - follow the lazy evaluation model of spark
 - can be named or unnamed
  - named will display thier running results in the spark ui
  - unnamed will not
 
 1. example
  - custom aggregation on the flight dataset, use the dataset api
  in Scala
  case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String, count: BigInt)
  val flights = spark.read
    .parquet("/data/flight-data/parquet/2010-summary.parquet")
    .as[Flight]
    
  in Python
  flights = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet")
  
  - create accumulator (count the number of flights to or from China)
  in Scala
  import org.apache.spark.util.LongAccumulator
  val accUnnamed = new LongAccumulator
  val acc = spark.sparkContext.register(accUnnamed)
  
  in Python
  accChina = spark.sparkContext.accumulator(0)

  - named accumulator
   - a short-hand method
   - long-hand method
  in Scala
  val accChina = new LongAccumulator
  val accChina2 = spark.sparkContext.longAccumulator("China")
  spark.sparkContext.register(accChina, "China")
  
  - define the way we add to our accumulator
  in Scala
  def accChinaFunc(flight_row: Flight) = {
   val destination = flight_row.DEST_COUNTRY_NAME
   val origin = flight_row.ORIGIN_COUNTRY_NAME
   if (destination == "China") {
    accChina.add(flight_row.count.toLong)
   }
   if (origin == "China") {
   accChina.add(flight_row.count.toLong)
   }
  }
  
  in Python
  def accChinaFunc(flight_row):
   destination = flight_row["DEST_COUNTRY_NAME"]
   origin = flight_row["ORIGIN_COUNTRY_NAME"]
   if destination == "China":
    accChina.add(flight_row["count"])
   if origin == "China":
    accChina.add(flight_row["count"])
    
  - iterate over every row in our flights via foreach method
  in Scala
  flights.foreach(flight_row => accChinaFunc(flight_row))
  
  in Python
  flights.foreach(lambda flight_row: accChinaFunc(flight_row))
  
 2. custom accumulators
  - scala : subclass the AccumulatorV2 class
  - python : create your own custom accumulators by subclassing AccumulatorParam
   
